Automated Headless Scraping of Spotify Editorial Playlists
Introduction and Challenges
Spotify’s editorial playlists (curated by Spotify, like New Music Friday or Today’s Top Hits) cannot be accessed via the public Web API for new third-party apps
reddit.com
reddit.com
. This means we must scrape these playlists as an authenticated user. The goal is a fully automated, no-manual-intervention system to fetch playlist data on-demand and on a schedule. Key challenges include: maintaining a persistent login (to avoid re-login each run), running in a headless server environment (no GUI/GPU), scheduling periodic jobs, and doing all of this in a secure, centralized way. We’ll design a robust solution that meets these needs.
Authentication & Persistent Session Strategy
To scrape private data like editorial playlists, we need an authenticated Spotify session. The recommended approach is to automate a one-time login in a headless browser and then reuse that session going forward
github.com
. Specifically:
Headless Login: Use a tool like Puppeteer or Playwright to launch a headless Chromium browser and simulate the Spotify login form (username/password). This initial step yields Spotify’s auth cookies and tokens.
Export & Store Cookies/Tokens: After login, extract the session cookies (especially the long-lived auth token cookie). For Spotify’s web, the sp_dc cookie is known to hold a refresh token or session token valid for a long time (often months). We store this token and other needed cookies securely. As one project notes, a typical method is to log in once and save the session data (cookies, local storage, etc.) to disk for reuse
browserless.io
browserless.io
. In our case, we’ll persist the Spotify auth cookies (and any refresh token) so that future runs don’t require logging in again manually.
Token Refresh: With the stored credentials, we can obtain fresh access tokens programmatically. Spotify’s web player provides an endpoint (e.g. open.spotify.com/api/token or formerly .../get_access_token?reason=transport&productType=web_player) which, when called with valid Spotify cookies, returns a new access token (plus expiration) for the logged-in user
awesome.ecosyste.ms
. We can call this in a lightweight script to get a Bearer token when needed, avoiding a full browser launch each time. The stored cookie (essentially a refresh token) remains valid for up to a year in many cases
github.com
, so truly manual re-authentication can be avoided. This session token injection method (using saved cookies to get API tokens) lets us mimic official web requests without using the official API (thus bypassing the API restrictions).
Centralized Auth: Only one Spotify account needs to perform this login. We designate a “service” Spotify account (or an admin user’s account) to handle all editorial playlist access. That account’s cookies/tokens are stored centrally and used for all scraping tasks. This way, multiple users of our system benefit from one authenticated session, and only one set of credentials/cookies needs management.
Headless Scraping Tools Comparison
To carry out the above, we have several tooling options:
Puppeteer (Node.js): A popular Node library for controlling Chrome/Chromium. Puppeteer runs headless by default and can automate the Spotify web login and page interactions. It’s well-supported and has plugins (e.g. stealth) to reduce detection. However, vanilla Puppeteer’s headless Chrome can be detectable by some sites without tweaks
reddit.com
. It’s powerful but somewhat heavyweight, and by default downloads a large Chromium binary (~170MB).
Playwright (Node/Python): A newer multi-language toolkit by Microsoft. Like Puppeteer, it controls headless browsers (Chromium, Firefox, WebKit) and offers robust features (auto-wait, better error handling). Playwright can be a bit less detectable out-of-the-box and includes utilities for handling multiple browser contexts easily. It also allows saving browser state (cookies/localStorage) to disk in one call (browserContext.storageState()), simplifying session persistence. Many find Playwright’s API flexible and modern, and it can run in Node or Python environments.
Headless Chrome with XVFB: This refers to running a real Chrome or browser in “headful” mode but invisible via a virtual framebuffer (Xvfb). In practice, Puppeteer/Playwright headless mode makes this unnecessary for most cases. Using Xvfb + Chrome might be needed if a site refuses to serve content to headless user agents – one could run a full browser in Xvfb to appear non-headless. This is more resource-intensive and complex (requiring a display server emulator), so it’s generally a fallback, not first choice.
Session Token Injection (No Browser for Data Fetch): As described, once we have cookies or a refresh token, we can avoid using a browser to retrieve data. Instead, use direct HTTP requests (e.g. via fetch or Axios in Node, or Python requests) to Spotify’s private endpoints. For example, after obtaining an access token via the cookie, we can call the internal Spotify API endpoint for playlists (the same one the web client uses) – e.g. https://api.spotify.com/v1/playlists/{playlistId} with an Authorization: Bearer <token> header – to get playlist details in JSON. This approach is extremely fast and lightweight compared to rendering pages. It essentially injects our session token into Spotify’s API calls. This method avoids heavy browser automation for each scrape, using the browser only to refresh the session occasionally. It’s ideal for headless server environments with tight resources.
Browserless.io (Headless Browser as a Service): Browserless is a cloud service providing hosted headless Chrome/Playwright instances via an API. Using it means you don’t run Chrome on your own container at all – you send scripts or commands to the Browserless API, which executes them and returns data. This can simplify setup on platforms where running Chrome is problematic. Browserless also offers features like session persistence and solving captchas
browserless.io
browserless.io
. The trade-off is that you’re sending your browsing (and potentially Spotify login credentials or cookies) through a third-party service, which has security implications. It also introduces network latency and potential cost if usage is high. However, for low-volume or difficult environments, it’s a convenient option. It essentially externalizes the Puppeteer/Playwright work – your code just connects to a remote browser via WebSocket or REST
browserless.io
.
Comparing these options: For our use case, a hybrid approach is best. We can use Playwright/Puppeteer for the login and rare interactive needs, and then use direct HTTP API calls for the routine playlist fetching. This gives us the reliability of a browser to handle Spotify’s login flow (which may include redirects or 2FA checks), but the efficiency of raw requests thereafter. Running our own headless browser (via Puppeteer/Playwright) gives us full control and keeps credentials in-house. Using a service like Browserless could be a fallback if our hosting environment cannot support running Chrome at all (for example, if Replit severely restricts it).
Recommended Tech Stack and Tools
Node.js with Playwright is an excellent choice for this project’s core:
Playwright can perform the Spotify login in a headless Chromium environment with ease, and we can save the authenticated state. It also integrates well with Node for making follow-up HTTP requests (or we could even use Playwright’s network request capabilities to call the playlist endpoint).
Node.js is convenient for scheduling (with packages like node-cron) and for using the Spotify web APIs (via fetch or Axios). It also aligns with tools like Puppeteer/Playwright, which are Node-centric (though Playwright Python is an option if Python is preferred).
We will use a database (e.g. PostgreSQL) to store the scraped playlist data and perhaps to store the auth cookies/tokens securely (alternatively, tokens could be stored in a secure file or key vault). Both Railway and Fly.io can provision a Postgres instance easily for persistence. The scraper service will write playlist track info and metadata to the DB on each run, enabling historical tracking or further processing.
The system can be organized into a couple of components: Auth Module (handles login, refresh tokens, cookie storage) and Scraper Module (given a playlist ID, it fetches the latest tracks and writes to DB). We might also include a simple API layer (e.g. an Express server) if we need to trigger scrapes on-demand via HTTP requests or to expose data to other services.
Why Playwright over Puppeteer? Both could work, but Playwright’s ability to easily persist browser state and its robust handling of web pages is a plus. It also has a growing community and supports headful debugging if needed. That said, Puppeteer is equally viable – we just may need to manually handle cookie storage. (In Puppeteer we’d launch with headless: true and args: ['--no-sandbox', '--disable-setuid-sandbox'] for container environments
medium.com
medium.com
, then use page.cookies() and page.setCookie() to save/load sessions.) Ultimately, the stack is Node + a headless browser library + Node for scheduling & HTTP calls. This will run on a Linux container in the cloud.
Securing and Storing Session Credentials
Persistent login state must be maintained across deployments or restarts. We achieve this by saving the session cookies/tokens obtained after login:
In a simple form, we can write the cookies to a file (e.g. cookies.json) on the container’s filesystem. This file would contain the sp_dc cookie and others from the Spotify domain
browserless.io
. On startup, our app can load this file and restore cookies into the browser or use them in HTTP requests. Playwright can also directly launch a browser context from a saved JSON state.
Since file storage on some platforms can be ephemeral (e.g. a Railway deploy might reset the filesystem), consider storing the cookie string in a database or a secure secrets store. For instance, the cookie could be stored in an encrypted form in our Postgres DB or a key-value store. Only the scraping service has the decryption key (provided via environment variable).
Security: The Spotify cookie/token is effectively a password — anyone with it can act as the account. So, restrict access to it. Use environment secrets for any static credentials. If storing in DB, encrypt it or at least limit who can read that table. Our architecture uses a single central account’s credentials, making it easier to secure one set of secrets.
No manual refresh: By storing Spotify’s refresh token (from sp_dc or from OAuth if used), we can programmatically renew the session. The headless login yields a refresh token valid ~1 year
github.com
. We can either:
Use it by hitting the Spotify token endpoint to get new access tokens whenever the old expires (likely every hour).
Or simply rely on the cookie in subsequent calls – Spotify will issue fresh short-lived tokens as needed when we call the internal API. In practice, calling the api/token endpoint with a valid cookie will always return a current access token. This means we might not even need to schedule a refresh; every time we scrape, we first call for a new token using our long-lived cookie.
To guard against the cookie expiring (say after a year or if credentials change), implement a re-login procedure: e.g. if our token endpoint call fails (HTTP 401), catch it and trigger the Puppeteer/Playwright login again (using the stored username/password or a refresh script) to obtain a new cookie. This way, the system self-heals without manual involvement.
Scheduling and Orchestration
We need both on-demand scrapes and weekly scheduled runs for each playlist:
On-Demand: We can expose a function or HTTP endpoint that triggers a scrape job for a given playlist ID. For example, an Express route /scrape?playlist={id} could enqueue a job. This allows users or other services to request an immediate update.
Scheduled (Cron) Jobs: For weekly runs, we have options. If using Railway, it has a native cron scheduling feature where you can configure tasks to run on a schedule in the deployment UI
northflank.com
northflank.com
. We could set up a cron job for each playlist or one job that iterates over all target playlists weekly. Alternatively, within our Node app we can use a scheduling library like node-cron or simple setInterval/setTimeout loops to trigger jobs on schedule. For example, if playlists update every Friday, schedule the job every Friday at a specific hour.
Per-Playlist vs Batch: If the playlists have different update times, we can stagger the cron schedule per playlist. Otherwise, a single weekly batch that goes through all playlists is easier to manage. Each job will retrieve the playlist’s data via the token method and then store/update the DB.
Avoiding Overlap: Ensure the scheduler doesn’t start a new scrape if the previous one for that playlist is still running (to prevent duplicate work or race conditions). This can be handled by using job locks or simply designing tasks that complete quickly (playlist fetch is quick, only a few seconds).
Cron on Platforms: On Railway, as noted, cron jobs are supported even on free tier
northflank.com
, but passing dynamic parameters can be tricky (their cron triggers a specific deploy command, so to vary playlist ID, we might instead schedule a general job and have the code loop multiple playlists)
northflank.com
. On Fly.io, there is no built-in cron, so we’d rely on the app’s internal scheduler or an external service to hit an endpoint. On Replit, always-on processes can run a scheduler in code, but Replit’s free plan no longer supports long-running apps continuously
blog.replit.com
 (they moved to a deployment model). Thus, in all scenarios, it might be simplest to run a single continuously running service that handles its own scheduling (especially if using Fly or a similar container host).
We can also integrate with external schedulers: e.g. GitHub Actions or a cron-as-a-service to send webhooks to our app on schedule. However, keeping it self-contained is more straightforward.
Deployment Platform Trade-Offs (Railway vs Replit vs Fly.io)
Replit: Replit is great for quick prototyping and has an in-browser IDE. However, running headless Chrome on Replit can be challenging. By default, Replit containers don’t include Chrome and have memory and security constraints. Users have reported difficulties due to missing libraries and resource limits (Chromium needs 300-500MB RAM and 64-bit libraries)
replit.discourse.group
replit.discourse.group
. It is possible – by using a Nix environment to install Chromium and launching Puppeteer with --no-sandbox – but it may be unreliable on the free tier. Also, Replit free apps won’t stay running continuously without the new Deployments feature or a paid plan (their old Always-On was removed)
blog.replit.com
. In short, Replit is good for development or light usage, but for a stable 24/7 scraper, it’s less ideal unless you upgrade. Railway: Railway is a container-based PaaS with an easy git-deploy workflow. It’s well-suited for Node apps and can handle Puppeteer/Playwright if you add the necessary dependencies in your build (you might use a Dockerfile or Railway’s build packs). Railway’s advantages:
Simplicity in deployment and built-in support for databases (Postgres, Redis, etc.) which we will likely use
northflank.com
.
A cron scheduler built-in for recurring jobs
northflank.com
, which fits our weekly scrape needs. Do note its limitations (no complex parameterization, etc.)
northflank.com
, but for “run this script once a week” it’s fine.
On the free plan, Railway gives some free hours ($5 credit); a weekly job would consume very little, but if the container runs 24/7 it will use those credits. It likely requires at least the lowest paid tier for continuous operation after credits are used
northflank.com
.
Railway containers can run headless Chrome. We’d ensure to use --no-sandbox since Railway containers run as root by default. Many have successfully used Puppeteer on Railway by including apt packages (like fonts, gconf, etc.) or relying on the bundled Chromium from Puppeteer.
Persistence: The file system resets on deploy, so we’d rely on storing cookies in a database or Railway’s built-in variable store (not ideal for dynamic data) for persistence.
Fly.io: Fly.io lets you deploy Docker images globally. It provides more low-level control and can run always-on processes easily. You can allocate more RAM/CPU as needed for Chrome. Fly would be a good choice if you need to run closer to certain regions (for example, if you need to scrape region-specific versions of playlists by running the scraper in different locales). Fly doesn’t have a native scheduler, but you could set up a cron within the container. Alternatively, you might deploy a Fly Machine that runs on a schedule and then sleeps (Fly Machines can be triggered to run on demand). Fly’s free tier offers a modest amount of CPU/RAM time which might cover a small weekly job, but if you keep a machine running constantly, you may exceed it. On Fly you also have the option of attaching a volume for persistent storage if you want to keep the cookie file locally instead of using a separate DB. Resource and OS Considerations: Both Railway and Fly allow you to pick an appropriate base image (e.g. use an official Node image or one of Playwright’s pre-built Docker images that include browsers). This makes it straightforward to include Chrome. Replit’s environment is a bit more fixed, but you can use Nix to install missing libraries. The lack of GPU on all these platforms is not a problem – Chrome’s software rendering in headless mode is sufficient for scraping. We do need to ensure we have enough RAM; headless browsers can spike in memory usage, so choosing a plan with at least 512MB (preferably 1GB) is wise for stability. Summary: For a production-grade solution, Railway or Fly.io are preferable to Replit. Railway is a bit more turnkey with its UI and cron jobs (great for a centralized scheduled scraper with minimal ops effort), whereas Fly offers more flexibility and potentially better performance if configured well. Replit could be used if one is already integrated into that ecosystem, but expect to spend time tweaking it to run headless Chrome, and likely need a paid plan to keep it running.
System Architecture and Workflow
Now let’s outline the overall architecture integrating all these pieces:
Authentication Service/Module: This component is responsible for ensuring we have a valid Spotify session. On startup, it checks for stored credentials (cookie/refresh token). If found, it attempts to use them (e.g. calling Spotify’s token endpoint to ensure they’re still valid). If not found or expired, it triggers a headless login sequence. The login uses stored Spotify credentials (username/password) – which are kept secure, e.g. in environment vars or a vault – and automates the OAuth web flow. Once logged in, it saves the updated cookies (especially the sp_dc refresh token cookie) to the database or file. This module can run at app start and periodically (if tokens expire after a long time) but ideally, thanks to the long lifespan, login is rare. One instance of this service can update the stored cookie, and all scrapers use the same cookie (so we don’t have multiple logins).
Scraper Job/Service: This does the actual playlist fetch. It can be the same process as the auth module or separate. The scraper will:
Request an access token by calling the internal API with the stored cookie (or by retrieving a stored access token if we keep it updated). With sp_dc and other cookies present, hitting open.spotify.com/api/token yields an access token JSON which we parse
awesome.ecosyste.ms
.
Call the Spotify web API for the playlist. For example, GET https://api.spotify.com/v1/playlists/{playlist_id} with header Authorization: Bearer <token>. This returns the playlist’s details and items (tracks) in JSON. We could also use the Spotify Web client API if needed (there are also internal endpoints like spclient.wg.spotify.com that the web might use for certain data). But the standard web API endpoint works as long as we use a legitimate user token. (Since we are effectively mimicking the official web application, we circumvent the 404 errors that a third-party API token would give for editorial playlists.)
Parse the JSON to extract track info, then upsert that into our database. We’ll likely have tables like playlists and playlist_tracks. The scraper can update the playlist’s metadata (name, last updated time, etc.) and then reconcile the tracks (add new ones, mark removed ones, etc.) depending on the use case. This constitutes the DB sync – ensuring the DB reflects the latest playlist content. If we want to track historical changes, we might store snapshots or changes rather than just the latest state.
Scheduler/Orchestrator: This is the piece that triggers the scraper at the right times. In a simple deployment, this could be just a cron process or a timed loop within the scraper service. For instance, using node-cron we schedule scraper.scrapePlaylist(id) to run every Monday at 9am for Playlist A, every Friday 12pm for Playlist B, etc., or all on a certain schedule. If using Railway’s cron feature, it might kick off a new instance of the service on schedule – that instance would need to know which playlist to process (perhaps via an environment variable or argument per job). A more advanced design could include a job queue (like BullMQ or a lightweight queue) where scheduled tasks enqueue scrape jobs that a worker process then executes, providing more control and the ability to scale out if needed. Given the likely modest scope (a handful of playlists), a single process can handle it.
API Layer (Optional): If we want on-demand triggers or to expose data, we might include an API. For example, a small web server could have endpoints like /playlists/{id}/refresh to trigger a refresh, or /playlists/{id} to return the latest data from the DB. This would allow a frontend or other service to request updates or fetch stored results. This API would use the same scraper module under the hood for the refresh action. Ensure that triggering on-demand scraping is authenticated or rate-limited if this service is public, to prevent abuse.
Everything is centrally managed, with one Spotify login for all tasks. The division of responsibilities ensures that we don’t re-authenticate for every playlist fetch (we do it once and reuse the session), and we don’t run a full browser for each scrape (we only do so when necessary). Most of the time, the weekly jobs will be just HTTP calls which are lightweight.
Trade-offs and Final Thoughts
With this setup, we achieve a zero-manual scraping pipeline:
No manual logins or cookie copying: The system either uses a stored long-lived cookie or can programmatically log in via headless browser when needed
github.com
. The refresh token mechanism means no clipboard copying of tokens after initial setup.
Centralized credentials: One account’s credentials are used and centrally stored, simplifying management. This account can be maintained by one admin user. If the cookie needs refreshing, that admin can trigger a re-login (or we automate it on expiration).
Headless environment compatibility: The chosen stack (Node + headless Chrome) runs in container environments without displays. Headless mode and --no-sandbox flags handle the lack of GUI. If using platforms like Replit or Railway, we’ve accounted for their constraints (installing Chromium via Nix or using base images with Chrome). For example, on Replit one might use a Nix configuration to add Chrome dependencies, since by default Chrome isn’t present
replit.discourse.group
. On Railway/Fly, use a Dockerfile that includes Chrome or use Playwright’s provided Docker image for convenience.
Scalability: For the given task (a few playlists, weekly), even a single small instance can handle it easily. But if the scope grows (many playlists or more frequent runs), we could scale out by running multiple scrapers in parallel or distributing tasks by region (Fly.io would shine if, say, you need to scrape region-specific editorial lists by deploying in those regions with region-specific accounts).
Maintenance: We should monitor if Spotify changes their web endpoints or login flow. Since we are not using the official API, such changes could break the scraper. Mitigate this by encapsulating the scraping logic so it’s easy to update (for example, if open.spotify.com/api/token moves or requires a new parameter, we can adjust in one place). Also, keep an eye on cookie expiration – possibly set a calendar reminder to rotate the Spotify account’s password or refresh token annually if needed.
By combining Playwright/Puppeteer for auth with direct API calls for data, scheduled via cron in an always-on Node service, deployed on a platform like Railway or Fly, we have a robust solution. It avoids manual work entirely and centralizes the heavy lifting, delivering fresh Spotify editorial playlist data securely and reliably.